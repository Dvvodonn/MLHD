{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLHD Model Evaluation Notebook\n",
    "\n",
    "Comprehensive evaluation of the YOLO-like detector with configurable hyperparameters and visualizations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "from glob import glob\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "ROOT = Path.cwd()\n",
    "sys.path.insert(0, str(ROOT))\n",
    "\n",
    "from models.yolo_like import Model\n",
    "from datasets.target_encoding import load_yolo_labels\n",
    "from datasets.transforms import letterbox_image\n",
    "from eval.evaluator import ObjectDetectionEvaluator, evaluate_predictions\n",
    "\n",
    "print(f\"Working directory: {ROOT}\")\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Hyperparameters Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Configuration\n",
    "CHECKPOINT_PATH = 'checkpoints/best.pt'\n",
    "GRID_SIZE = 26\n",
    "IMG_SIZE = 416\n",
    "\n",
    "# Inference Hyperparameters\n",
    "CONFIDENCE_THRESHOLD = 0.5  # Minimum confidence for detections\n",
    "NMS_IOU_THRESHOLD = 0.5     # IoU threshold for Non-Maximum Suppression\n",
    "\n",
    "# Evaluation Hyperparameters\n",
    "EVAL_IOU_THRESHOLDS = [0.5, 0.75]  # IoU thresholds for metrics computation\n",
    "\n",
    "# Dataset Paths\n",
    "VAL_IMAGES_DIR = 'data/processed_training_3/images/val'\n",
    "VAL_LABELS_DIR = 'data/processed_training_3/labels/val'\n",
    "\n",
    "# Output Configuration\n",
    "OUTPUT_DIR = 'outputs/evaluation'\n",
    "MAX_IMAGES = None  # Set to a number to limit evaluation (None = all images)\n",
    "\n",
    "# Device Configuration\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available():\n",
    "    DEVICE = torch.device('mps')\n",
    "else:\n",
    "    DEVICE = torch.device('cpu')\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"HYPERPARAMETERS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"Grid Size: {GRID_SIZE}\")\n",
    "print(f\"Image Size: {IMG_SIZE}\")\n",
    "print(f\"Confidence Threshold: {CONFIDENCE_THRESHOLD}\")\n",
    "print(f\"NMS IoU Threshold: {NMS_IOU_THRESHOLD}\")\n",
    "print(f\"Evaluation IoU Thresholds: {EVAL_IOU_THRESHOLDS}\")\n",
    "print(f\"Device: {DEVICE}\")\n",
    "print(f\"Max Images: {MAX_IMAGES if MAX_IMAGES else 'All'}\")\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\nLoading model...\")\nmodel = Model(S=GRID_SIZE)\n\n# Load checkpoint to CPU first to avoid MPS compatibility issues\ncheckpoint = torch.load(CHECKPOINT_PATH, map_location='cpu')\nmodel.load_state_dict(checkpoint['model'])\n\n# Then move to target device\nmodel.to(DEVICE)\nmodel.eval()\n\nprint(f\"Model loaded from {CHECKPOINT_PATH}\")\nprint(f\"Checkpoint epoch: {checkpoint.get('epoch', 'N/A')}\")\nprint(f\"Checkpoint validation loss: {checkpoint.get('val_loss', 'N/A'):.4f}\")\n\n# Count model parameters\ntotal_params = sum(p.numel() for p in model.parameters())\ntrainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(f\"\\nModel Parameters:\")\nprint(f\"  Total: {total_params:,}\")\nprint(f\"  Trainable: {trainable_params:,}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_predictions(pred_grid, conf_thresh, grid_size, img_size):\n",
    "    \"\"\"Decode grid predictions [S, S, 5] to bounding boxes.\"\"\"\n",
    "    S = grid_size\n",
    "    boxes = []\n",
    "\n",
    "    for j in range(S):\n",
    "        for i in range(S):\n",
    "            tx = pred_grid[j, i, 0].item()\n",
    "            ty = pred_grid[j, i, 1].item()\n",
    "            tw = pred_grid[j, i, 2].item()\n",
    "            th = pred_grid[j, i, 3].item()\n",
    "            obj_conf = pred_grid[j, i, 4].item()\n",
    "\n",
    "            if obj_conf < conf_thresh:\n",
    "                continue\n",
    "\n",
    "            cx = (i + tx) / S\n",
    "            cy = (j + ty) / S\n",
    "            w = tw\n",
    "            h = th\n",
    "\n",
    "            cx_pix = cx * img_size\n",
    "            cy_pix = cy * img_size\n",
    "            w_pix = w * img_size\n",
    "            h_pix = h * img_size\n",
    "\n",
    "            x1 = cx_pix - w_pix / 2\n",
    "            y1 = cy_pix - h_pix / 2\n",
    "            x2 = cx_pix + w_pix / 2\n",
    "            y2 = cy_pix + h_pix / 2\n",
    "\n",
    "            boxes.append((x1, y1, x2, y2, obj_conf))\n",
    "\n",
    "    return boxes\n",
    "\n",
    "\n",
    "def compute_iou(box1, box2):\n",
    "    \"\"\"Compute IoU between two boxes.\"\"\"\n",
    "    x1_1, y1_1, x2_1, y2_1, _ = box1\n",
    "    x1_2, y1_2, x2_2, y2_2, _ = box2\n",
    "\n",
    "    x1_i = max(x1_1, x1_2)\n",
    "    y1_i = max(y1_1, y1_2)\n",
    "    x2_i = min(x2_1, x2_2)\n",
    "    y2_i = min(y2_1, y2_2)\n",
    "\n",
    "    if x2_i <= x1_i or y2_i <= y1_i:\n",
    "        return 0.0\n",
    "\n",
    "    inter_area = (x2_i - x1_i) * (y2_i - y1_i)\n",
    "    area1 = (x2_1 - x1_1) * (y2_1 - y1_1)\n",
    "    area2 = (x2_2 - x1_2) * (y2_2 - y1_2)\n",
    "    union_area = area1 + area2 - inter_area\n",
    "\n",
    "    return inter_area / union_area if union_area > 0 else 0.0\n",
    "\n",
    "\n",
    "def nms(boxes, iou_thresh):\n",
    "    \"\"\"Apply Non-Maximum Suppression.\"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    boxes = sorted(boxes, key=lambda x: x[4], reverse=True)\n",
    "    keep = []\n",
    "\n",
    "    while len(boxes) > 0:\n",
    "        best = boxes[0]\n",
    "        keep.append(best)\n",
    "        boxes = boxes[1:]\n",
    "\n",
    "        filtered = []\n",
    "        for box in boxes:\n",
    "            iou = compute_iou(best, box)\n",
    "            if iou < iou_thresh:\n",
    "                filtered.append(box)\n",
    "        boxes = filtered\n",
    "\n",
    "    return keep\n",
    "\n",
    "\n",
    "def transform_boxes_to_original(boxes, params, img_size):\n",
    "    \"\"\"Transform boxes from letterbox coordinates to original image coordinates.\"\"\"\n",
    "    if len(boxes) == 0:\n",
    "        return []\n",
    "\n",
    "    scale = params['scale']\n",
    "    pad_w = params['pad_w']\n",
    "    pad_h = params['pad_h']\n",
    "    orig_w, orig_h = params['orig_wh']\n",
    "\n",
    "    transformed = []\n",
    "    for (x1, y1, x2, y2, conf) in boxes:\n",
    "        x1_no_pad = x1 - pad_w\n",
    "        y1_no_pad = y1 - pad_h\n",
    "        x2_no_pad = x2 - pad_w\n",
    "        y2_no_pad = y2 - pad_h\n",
    "\n",
    "        x1_orig = x1_no_pad / scale\n",
    "        y1_orig = y1_no_pad / scale\n",
    "        x2_orig = x2_no_pad / scale\n",
    "        y2_orig = y2_no_pad / scale\n",
    "\n",
    "        x1_orig = max(0, min(x1_orig, orig_w))\n",
    "        y1_orig = max(0, min(y1_orig, orig_h))\n",
    "        x2_orig = max(0, min(x2_orig, orig_w))\n",
    "        y2_orig = max(0, min(y2_orig, orig_h))\n",
    "\n",
    "        transformed.append((x1_orig, y1_orig, x2_orig, y2_orig, conf))\n",
    "\n",
    "    return transformed\n",
    "\n",
    "\n",
    "def infer_image(model, image_path, device, img_size, grid_size, conf_thresh, iou_thresh):\n",
    "    \"\"\"Run inference on a single image.\"\"\"\n",
    "    img_tensor, params = letterbox_image(image_path, (img_size, img_size))\n",
    "    img_tensor = img_tensor.unsqueeze(0).to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pred = model(img_tensor)\n",
    "\n",
    "    boxes = decode_predictions(pred[0], conf_thresh, grid_size, img_size)\n",
    "    boxes = nms(boxes, iou_thresh)\n",
    "\n",
    "    return boxes, params\n",
    "\n",
    "print(\"Helper functions loaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Run Evaluation on Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_paths = sorted(glob(os.path.join(VAL_IMAGES_DIR, \"*.jpg\")))\n",
    "\n",
    "if MAX_IMAGES:\n",
    "    image_paths = image_paths[:MAX_IMAGES]\n",
    "\n",
    "all_predictions = []\n",
    "all_ground_truth = []\n",
    "image_ids = []\n",
    "stats = {\n",
    "    'total': 0,\n",
    "    'processed': 0,\n",
    "    'errors': 0,\n",
    "    'images_with_objects': 0,\n",
    "    'total_pred_objects': 0,\n",
    "    'total_gt_objects': 0\n",
    "}\n",
    "\n",
    "print(f\"\\nEvaluating {len(image_paths)} images...\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for idx, img_path in enumerate(image_paths):\n",
    "    stats['total'] += 1\n",
    "\n",
    "    try:\n",
    "        basename = os.path.basename(img_path)\n",
    "        label_name = os.path.splitext(basename)[0] + '.txt'\n",
    "        label_path = os.path.join(VAL_LABELS_DIR, label_name)\n",
    "\n",
    "        gt_boxes = load_yolo_labels(label_path)\n",
    "        if len(gt_boxes) > 0:\n",
    "            stats['images_with_objects'] += 1\n",
    "        stats['total_gt_objects'] += len(gt_boxes)\n",
    "\n",
    "        img = cv2.imread(img_path)\n",
    "        h, w = img.shape[:2]\n",
    "        gt_pixel = []\n",
    "        for (cx, cy, bw, bh) in gt_boxes:\n",
    "            x1 = (cx - bw / 2) * w\n",
    "            y1 = (cy - bh / 2) * h\n",
    "            x2 = (cx + bw / 2) * w\n",
    "            y2 = (cy + bh / 2) * h\n",
    "            gt_pixel.append((x1, y1, x2, y2))\n",
    "\n",
    "        preds, params = infer_image(\n",
    "            model, img_path, DEVICE, IMG_SIZE, GRID_SIZE,\n",
    "            CONFIDENCE_THRESHOLD, NMS_IOU_THRESHOLD\n",
    "        )\n",
    "\n",
    "        preds_orig = transform_boxes_to_original(preds, params, IMG_SIZE)\n",
    "\n",
    "        all_predictions.append(preds_orig)\n",
    "        all_ground_truth.append(gt_pixel)\n",
    "        image_ids.append(basename)\n",
    "\n",
    "        stats['processed'] += 1\n",
    "        stats['total_pred_objects'] += len(preds_orig)\n",
    "\n",
    "        if (idx + 1) % 50 == 0:\n",
    "            print(f\"Processed {idx + 1}/{len(image_paths)} images...\")\n",
    "\n",
    "    except Exception as e:\n",
    "        stats['errors'] += 1\n",
    "        print(f\"Error processing {img_path}: {e}\")\n",
    "\n",
    "print(\"-\" * 70)\n",
    "print(f\"Evaluation complete: {stats['processed']} images processed, {stats['errors']} errors\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DATASET STATISTICS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total images evaluated: {stats['processed']}\")\n",
    "print(f\"Images with objects: {stats['images_with_objects']}\")\n",
    "print(f\"Total ground truth objects: {stats['total_gt_objects']}\")\n",
    "print(f\"Total predicted objects: {stats['total_pred_objects']}\")\n",
    "\n",
    "if stats['total_gt_objects'] > 0:\n",
    "    recall_ceiling = stats['total_pred_objects'] / stats['total_gt_objects']\n",
    "    print(f\"Recall ceiling (pred/gt): {recall_ceiling:.2%}\")\n",
    "\n",
    "print(f\"Average objects per image (GT): {stats['total_gt_objects'] / max(1, stats['processed']):.2f}\")\n",
    "print(f\"Average objects per image (Pred): {stats['total_pred_objects'] / max(1, stats['processed']):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Compute Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EVALUATION METRICS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "results = evaluate_predictions(\n",
    "    all_predictions, all_ground_truth,\n",
    "    iou_thresholds=EVAL_IOU_THRESHOLDS,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"MEAN AVERAGE PRECISION (mAP)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "evaluator = ObjectDetectionEvaluator(EVAL_IOU_THRESHOLDS)\n",
    "map_scores = evaluator.compute_map(all_predictions, all_ground_truth)\n",
    "\n",
    "print(\"\\nmAP scores:\")\n",
    "for iou_thresh, ap in map_scores.items():\n",
    "    print(f\"  IoU {iou_thresh}: {ap:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Detailed Results Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"DETAILED RESULTS SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for iou_thresh in sorted(results.keys()):\n",
    "    metrics = results[iou_thresh]\n",
    "    print(f\"\\nIoU Threshold: {iou_thresh}\")\n",
    "    print(f\"  Precision: {metrics.precision:.4f}\")\n",
    "    print(f\"  Recall:    {metrics.recall:.4f}\")\n",
    "    print(f\"  F1 Score:  {metrics.f1:.4f}\")\n",
    "    print(f\"  AP:        {metrics.ap:.4f}\")\n",
    "    print(f\"  TP: {metrics.tp}, FP: {metrics.fp}, FN: {metrics.fn}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Per-Image Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"PER-IMAGE PERFORMANCE\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "per_image_stats = []\n",
    "iou_05_threshold = 0.5\n",
    "\n",
    "for i, (preds, gts, img_id) in enumerate(zip(all_predictions, all_ground_truth, image_ids)):\n",
    "    tp, fp, fn, _ = evaluator.match_predictions_to_ground_truth(\n",
    "        preds, gts, iou_05_threshold\n",
    "    )\n",
    "    precision, recall, f1 = evaluator.compute_precision_recall_f1(tp, fp, fn)\n",
    "    per_image_stats.append({\n",
    "        'image_id': img_id,\n",
    "        'pred': len(preds),\n",
    "        'gt': len(gts),\n",
    "        'tp': tp,\n",
    "        'fp': fp,\n",
    "        'fn': fn,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    })\n",
    "\n",
    "per_image_stats.sort(key=lambda x: x['f1'], reverse=True)\n",
    "\n",
    "print(\"\\nTop 10 best performing images (by F1 score):\")\n",
    "print(f\"{'Image':<30} {'Pred':<5} {'GT':<5} {'TP':<4} {'FP':<4} {'FN':<4} {'F1':<8}\")\n",
    "print(\"-\" * 70)\n",
    "for stat in per_image_stats[:10]:\n",
    "    print(f\"{stat['image_id']:<30} {stat['pred']:<5} {stat['gt']:<5} \"\n",
    "          f\"{stat['tp']:<4} {stat['fp']:<4} {stat['fn']:<4} \"\n",
    "          f\"{stat['f1']:<8.4f}\")\n",
    "\n",
    "print(\"\\nBottom 10 worst performing images (by F1 score):\")\n",
    "print(f\"{'Image':<30} {'Pred':<5} {'GT':<5} {'TP':<4} {'FP':<4} {'FN':<4} {'F1':<8}\")\n",
    "print(\"-\" * 70)\n",
    "for stat in per_image_stats[-10:]:\n",
    "    print(f\"{stat['image_id']:<30} {stat['pred']:<5} {stat['gt']:<5} \"\n",
    "          f\"{stat['tp']:<4} {stat['fp']:<4} {stat['fn']:<4} \"\n",
    "          f\"{stat['f1']:<8.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Visualize Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "iou_thresholds = sorted(results.keys())\n",
    "precisions = [results[t].precision for t in iou_thresholds]\n",
    "recalls = [results[t].recall for t in iou_thresholds]\n",
    "f1_scores = [results[t].f1 for t in iou_thresholds]\n",
    "aps = [results[t].ap for t in iou_thresholds]\n",
    "\n",
    "axes[0, 0].bar(range(len(iou_thresholds)), precisions, color='blue', alpha=0.7)\n",
    "axes[0, 0].set_xticks(range(len(iou_thresholds)))\n",
    "axes[0, 0].set_xticklabels([f\"{t:.2f}\" for t in iou_thresholds])\n",
    "axes[0, 0].set_ylabel('Precision')\n",
    "axes[0, 0].set_xlabel('IoU Threshold')\n",
    "axes[0, 0].set_title('Precision vs IoU Threshold')\n",
    "axes[0, 0].set_ylim([0, 1])\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[0, 1].bar(range(len(iou_thresholds)), recalls, color='green', alpha=0.7)\n",
    "axes[0, 1].set_xticks(range(len(iou_thresholds)))\n",
    "axes[0, 1].set_xticklabels([f\"{t:.2f}\" for t in iou_thresholds])\n",
    "axes[0, 1].set_ylabel('Recall')\n",
    "axes[0, 1].set_xlabel('IoU Threshold')\n",
    "axes[0, 1].set_title('Recall vs IoU Threshold')\n",
    "axes[0, 1].set_ylim([0, 1])\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 0].bar(range(len(iou_thresholds)), f1_scores, color='orange', alpha=0.7)\n",
    "axes[1, 0].set_xticks(range(len(iou_thresholds)))\n",
    "axes[1, 0].set_xticklabels([f\"{t:.2f}\" for t in iou_thresholds])\n",
    "axes[1, 0].set_ylabel('F1 Score')\n",
    "axes[1, 0].set_xlabel('IoU Threshold')\n",
    "axes[1, 0].set_title('F1 Score vs IoU Threshold')\n",
    "axes[1, 0].set_ylim([0, 1])\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "axes[1, 1].bar(range(len(iou_thresholds)), aps, color='red', alpha=0.7)\n",
    "axes[1, 1].set_xticks(range(len(iou_thresholds)))\n",
    "axes[1, 1].set_xticklabels([f\"{t:.2f}\" for t in iou_thresholds])\n",
    "axes[1, 1].set_ylabel('Average Precision')\n",
    "axes[1, 1].set_xlabel('IoU Threshold')\n",
    "axes[1, 1].set_title('AP vs IoU Threshold')\n",
    "axes[1, 1].set_ylim([0, 1])\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'metrics_visualization.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nVisualization saved to {OUTPUT_DIR}/metrics_visualization.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. F1 Score Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f1_values = [stat['f1'] for stat in per_image_stats]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(f1_values, bins=20, color='purple', alpha=0.7, edgecolor='black')\n",
    "plt.xlabel('F1 Score')\n",
    "plt.ylabel('Number of Images')\n",
    "plt.title('Distribution of F1 Scores Across Images')\n",
    "plt.axvline(np.mean(f1_values), color='red', linestyle='--', linewidth=2, label=f'Mean: {np.mean(f1_values):.3f}')\n",
    "plt.axvline(np.median(f1_values), color='green', linestyle='--', linewidth=2, label=f'Median: {np.median(f1_values):.3f}')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'f1_distribution.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"F1 Score Statistics:\")\n",
    "print(f\"  Mean: {np.mean(f1_values):.4f}\")\n",
    "print(f\"  Median: {np.median(f1_values):.4f}\")\n",
    "print(f\"  Std Dev: {np.std(f1_values):.4f}\")\n",
    "print(f\"  Min: {np.min(f1_values):.4f}\")\n",
    "print(f\"  Max: {np.max(f1_values):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Visualize Sample Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_samples = min(6, len(image_ids))\n",
    "sample_indices = np.linspace(0, len(image_ids) - 1, num_samples, dtype=int)\n",
    "\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, sample_idx in enumerate(sample_indices):\n",
    "    img_id = image_ids[sample_idx]\n",
    "    img_path = os.path.join(VAL_IMAGES_DIR, img_id)\n",
    "    \n",
    "    img = cv2.imread(img_path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    \n",
    "    axes[idx].imshow(img)\n",
    "    \n",
    "    for (x1, y1, x2, y2) in all_ground_truth[sample_idx]:\n",
    "        rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='green', facecolor='none', label='GT')\n",
    "        axes[idx].add_patch(rect)\n",
    "    \n",
    "    for (x1, y1, x2, y2, conf) in all_predictions[sample_idx]:\n",
    "        rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none', linestyle='--', label='Pred')\n",
    "        axes[idx].add_patch(rect)\n",
    "    \n",
    "    stat = per_image_stats[sample_idx]\n",
    "    axes[idx].set_title(f\"{img_id}\\nF1: {stat['f1']:.3f} | P: {len(all_predictions[sample_idx])} GT: {len(all_ground_truth[sample_idx])}\")\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "handles = [plt.Line2D([0], [0], color='green', linewidth=2, label='Ground Truth'),\n",
    "           plt.Line2D([0], [0], color='red', linewidth=2, linestyle='--', label='Prediction')]\n",
    "fig.legend(handles=handles, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 0.98))\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'sample_predictions.png'), dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSample predictions visualization saved to {OUTPUT_DIR}/sample_predictions.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results to File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "with open(os.path.join(OUTPUT_DIR, 'evaluation_summary.txt'), 'w') as f:\n",
    "    f.write(\"=\"*70 + \"\\n\")\n",
    "    f.write(\"MLHD MODEL EVALUATION SUMMARY\\n\")\n",
    "    f.write(\"=\"*70 + \"\\n\\n\")\n",
    "\n",
    "    f.write(\"Configuration:\\n\")\n",
    "    f.write(f\"  Checkpoint: {CHECKPOINT_PATH}\\n\")\n",
    "    f.write(f\"  Confidence threshold: {CONFIDENCE_THRESHOLD}\\n\")\n",
    "    f.write(f\"  NMS IoU threshold: {NMS_IOU_THRESHOLD}\\n\")\n",
    "    f.write(f\"  Evaluation IoU thresholds: {EVAL_IOU_THRESHOLDS}\\n\\n\")\n",
    "\n",
    "    f.write(\"Results:\\n\")\n",
    "    for iou_thresh in sorted(results.keys()):\n",
    "        metrics = results[iou_thresh]\n",
    "        f.write(f\"\\nIoU {iou_thresh}:\\n\")\n",
    "        f.write(f\"  Precision: {metrics.precision:.4f}\\n\")\n",
    "        f.write(f\"  Recall:    {metrics.recall:.4f}\\n\")\n",
    "        f.write(f\"  F1 Score:  {metrics.f1:.4f}\\n\")\n",
    "        f.write(f\"  AP:        {metrics.ap:.4f}\\n\")\n",
    "        f.write(f\"  TP: {metrics.tp}, FP: {metrics.fp}, FN: {metrics.fn}\\n\")\n",
    "\n",
    "    f.write(f\"\\nmean Average Precision (mAP):\\n\")\n",
    "    for iou_thresh, ap in map_scores.items():\n",
    "        f.write(f\"  IoU {iou_thresh}: {ap:.4f}\\n\")\n",
    "\n",
    "    f.write(f\"\\nDataset Statistics:\\n\")\n",
    "    f.write(f\"  Total images: {stats['processed']}\\n\")\n",
    "    f.write(f\"  Images with objects: {stats['images_with_objects']}\\n\")\n",
    "    f.write(f\"  Total GT objects: {stats['total_gt_objects']}\\n\")\n",
    "    f.write(f\"  Total predicted objects: {stats['total_pred_objects']}\\n\")\n",
    "\n",
    "print(f\"\\nResults saved to {OUTPUT_DIR}/evaluation_summary.txt\")\n",
    "print(f\"\\nEvaluation complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 13. Training/Validation Loss Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check if training history is available in checkpoint\nif 'train_losses' in checkpoint and 'val_losses' in checkpoint:\n    train_losses = checkpoint['train_losses']\n    val_losses = checkpoint['val_losses']\n    epochs = range(1, len(train_losses) + 1)\n    \n    plt.figure(figsize=(12, 5))\n    \n    plt.subplot(1, 2, 1)\n    plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n    plt.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss')\n    plt.title('Training and Validation Loss over Epochs')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.subplot(1, 2, 2)\n    plt.plot(epochs, train_losses, 'b-', label='Training Loss', linewidth=2)\n    plt.plot(epochs, val_losses, 'r-', label='Validation Loss', linewidth=2)\n    plt.xlabel('Epoch')\n    plt.ylabel('Loss (log scale)')\n    plt.yscale('log')\n    plt.title('Training and Validation Loss (Log Scale)')\n    plt.legend()\n    plt.grid(True, alpha=0.3)\n    \n    plt.tight_layout()\n    plt.savefig(os.path.join(OUTPUT_DIR, 'loss_curves.png'), dpi=150, bbox_inches='tight')\n    plt.show()\n    \n    print(f\"Final Training Loss: {train_losses[-1]:.4f}\")\n    print(f\"Final Validation Loss: {val_losses[-1]:.4f}\")\n    print(f\"Best Validation Loss: {min(val_losses):.4f} (Epoch {val_losses.index(min(val_losses)) + 1})\")\nelse:\n    print(\"Training history not found in checkpoint. Skipping loss curves.\")\n    print(\"Note: To save training history, modify your training script to include:\")\n    print(\"  checkpoint['train_losses'] = train_losses\")\n    print(\"  checkpoint['val_losses'] = val_losses\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Precision-Recall Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Compute precision-recall curve by varying confidence threshold\nconf_thresholds = np.linspace(0.1, 0.9, 20)\npr_data = {'0.5': {'precisions': [], 'recalls': []}, '0.75': {'precisions': [], 'recalls': []}}\n\nprint(\"Computing Precision-Recall curves...\")\nprint(\"This may take a few minutes...\")\n\nfor conf_thresh in conf_thresholds:\n    # Re-run inference with different confidence thresholds\n    temp_predictions = []\n    \n    for img_path in image_paths[:len(all_predictions)]:\n        try:\n            preds, params = infer_image(\n                model, img_path, DEVICE, IMG_SIZE, GRID_SIZE,\n                conf_thresh, NMS_IOU_THRESHOLD\n            )\n            preds_orig = transform_boxes_to_original(preds, params, IMG_SIZE)\n            temp_predictions.append(preds_orig)\n        except:\n            temp_predictions.append([])\n    \n    # Compute metrics at IoU 0.5 and 0.75\n    for iou_thresh in [0.5, 0.75]:\n        total_tp = 0\n        total_fp = 0\n        total_fn = 0\n        \n        for preds, gts in zip(temp_predictions, all_ground_truth):\n            tp, fp, fn, _ = evaluator.match_predictions_to_ground_truth(preds, gts, iou_thresh)\n            total_tp += tp\n            total_fp += fp\n            total_fn += fn\n        \n        precision = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0\n        recall = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0\n        \n        pr_data[str(iou_thresh)]['precisions'].append(precision)\n        pr_data[str(iou_thresh)]['recalls'].append(recall)\n\n# Plot P-R curves\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(pr_data['0.5']['recalls'], pr_data['0.5']['precisions'], 'b-o', linewidth=2, markersize=4)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve (IoU=0.5)')\nplt.grid(True, alpha=0.3)\nplt.xlim([0, 1])\nplt.ylim([0, 1])\n\nplt.subplot(1, 2, 2)\nplt.plot(pr_data['0.75']['recalls'], pr_data['0.75']['precisions'], 'r-o', linewidth=2, markersize=4)\nplt.xlabel('Recall')\nplt.ylabel('Precision')\nplt.title('Precision-Recall Curve (IoU=0.75)')\nplt.grid(True, alpha=0.3)\nplt.xlim([0, 1])\nplt.ylim([0, 1])\n\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, 'pr_curves.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(\"Precision-Recall curves saved.\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 15. Confidence Score Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Analyze confidence scores of predictions\nall_confidences = []\ntp_confidences = []\nfp_confidences = []\n\niou_thresh = 0.5\n\nfor preds, gts in zip(all_predictions, all_ground_truth):\n    for pred in preds:\n        conf = pred[4]\n        all_confidences.append(conf)\n        \n        # Check if this prediction is TP or FP\n        is_tp = False\n        for gt in gts:\n            pred_box = pred[:4]\n            gt_box = gt[:4]\n            \n            # Compute IoU\n            x1_i = max(pred_box[0], gt_box[0])\n            y1_i = max(pred_box[1], gt_box[1])\n            x2_i = min(pred_box[2], gt_box[2])\n            y2_i = min(pred_box[3], gt_box[3])\n            \n            if x2_i > x1_i and y2_i > y1_i:\n                inter_area = (x2_i - x1_i) * (y2_i - y1_i)\n                pred_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n                gt_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n                union_area = pred_area + gt_area - inter_area\n                iou = inter_area / union_area if union_area > 0 else 0\n                \n                if iou >= iou_thresh:\n                    is_tp = True\n                    break\n        \n        if is_tp:\n            tp_confidences.append(conf)\n        else:\n            fp_confidences.append(conf)\n\n# Plot confidence distributions\nfig, axes = plt.subplots(1, 3, figsize=(18, 5))\n\naxes[0].hist(all_confidences, bins=30, color='blue', alpha=0.7, edgecolor='black')\naxes[0].axvline(CONFIDENCE_THRESHOLD, color='red', linestyle='--', linewidth=2, label=f'Threshold: {CONFIDENCE_THRESHOLD}')\naxes[0].set_xlabel('Confidence Score')\naxes[0].set_ylabel('Number of Predictions')\naxes[0].set_title('All Predictions - Confidence Distribution')\naxes[0].legend()\naxes[0].grid(True, alpha=0.3)\n\naxes[1].hist(tp_confidences, bins=30, color='green', alpha=0.7, edgecolor='black')\naxes[1].axvline(CONFIDENCE_THRESHOLD, color='red', linestyle='--', linewidth=2, label=f'Threshold: {CONFIDENCE_THRESHOLD}')\naxes[1].set_xlabel('Confidence Score')\naxes[1].set_ylabel('Number of Predictions')\naxes[1].set_title(f'True Positives - Confidence Distribution (IoU\u2265{iou_thresh})')\naxes[1].legend()\naxes[1].grid(True, alpha=0.3)\n\naxes[2].hist(fp_confidences, bins=30, color='red', alpha=0.7, edgecolor='black')\naxes[2].axvline(CONFIDENCE_THRESHOLD, color='red', linestyle='--', linewidth=2, label=f'Threshold: {CONFIDENCE_THRESHOLD}')\naxes[2].set_xlabel('Confidence Score')\naxes[2].set_ylabel('Number of Predictions')\naxes[2].set_title(f'False Positives - Confidence Distribution (IoU<{iou_thresh})')\naxes[2].legend()\naxes[2].grid(True, alpha=0.3)\n\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, 'confidence_distribution.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nConfidence Statistics:\")\nprint(f\"  All Predictions - Mean: {np.mean(all_confidences):.3f}, Median: {np.median(all_confidences):.3f}\")\nprint(f\"  True Positives  - Mean: {np.mean(tp_confidences):.3f}, Median: {np.median(tp_confidences):.3f}\")\nprint(f\"  False Positives - Mean: {np.mean(fp_confidences):.3f}, Median: {np.median(fp_confidences):.3f}\")\nprint(f\"  Total Predictions: {len(all_confidences)}\")\nprint(f\"  True Positives: {len(tp_confidences)} ({100*len(tp_confidences)/len(all_confidences):.1f}%)\")\nprint(f\"  False Positives: {len(fp_confidences)} ({100*len(fp_confidences)/len(all_confidences):.1f}%)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 16. Failure Case Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Categorize failure cases\nfailure_cases = {\n    'missed_detections': [],  # FN: Objects in GT but not detected\n    'false_positives': [],     # FP: Predictions with no matching GT\n    'low_confidence_tp': [],   # TP with low confidence\n    'high_confidence_fp': []   # FP with high confidence\n}\n\niou_threshold = 0.5\nlow_conf_thresh = 0.3\nhigh_conf_thresh = 0.7\n\nfor idx, (preds, gts, img_id) in enumerate(zip(all_predictions, all_ground_truth, image_ids)):\n    # Find missed detections (FN)\n    matched_gts = set()\n    \n    for pred in preds:\n        for gt_idx, gt in enumerate(gts):\n            pred_box = pred[:4]\n            gt_box = gt[:4]\n            \n            x1_i = max(pred_box[0], gt_box[0])\n            y1_i = max(pred_box[1], gt_box[1])\n            x2_i = min(pred_box[2], gt_box[2])\n            y2_i = min(pred_box[3], gt_box[3])\n            \n            if x2_i > x1_i and y2_i > y1_i:\n                inter_area = (x2_i - x1_i) * (y2_i - y1_i)\n                pred_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n                gt_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n                union_area = pred_area + gt_area - inter_area\n                iou = inter_area / union_area if union_area > 0 else 0\n                \n                if iou >= iou_threshold:\n                    matched_gts.add(gt_idx)\n                    \n                    # Check for low confidence TP\n                    if pred[4] < low_conf_thresh:\n                        failure_cases['low_confidence_tp'].append({\n                            'image': img_id,\n                            'confidence': pred[4],\n                            'iou': iou\n                        })\n    \n    # Unmatched GTs are missed detections\n    for gt_idx in range(len(gts)):\n        if gt_idx not in matched_gts:\n            failure_cases['missed_detections'].append({\n                'image': img_id,\n                'gt_box': gts[gt_idx]\n            })\n    \n    # Check for false positives\n    for pred in preds:\n        is_matched = False\n        best_iou = 0\n        \n        for gt in gts:\n            pred_box = pred[:4]\n            gt_box = gt[:4]\n            \n            x1_i = max(pred_box[0], gt_box[0])\n            y1_i = max(pred_box[1], gt_box[1])\n            x2_i = min(pred_box[2], gt_box[2])\n            y2_i = min(pred_box[3], gt_box[3])\n            \n            if x2_i > x1_i and y2_i > y1_i:\n                inter_area = (x2_i - x1_i) * (y2_i - y1_i)\n                pred_area = (pred_box[2] - pred_box[0]) * (pred_box[3] - pred_box[1])\n                gt_area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n                union_area = pred_area + gt_area - inter_area\n                iou = inter_area / union_area if union_area > 0 else 0\n                best_iou = max(best_iou, iou)\n                \n                if iou >= iou_threshold:\n                    is_matched = True\n                    break\n        \n        if not is_matched:\n            failure_cases['false_positives'].append({\n                'image': img_id,\n                'confidence': pred[4],\n                'best_iou': best_iou\n            })\n            \n            # High confidence FP\n            if pred[4] >= high_conf_thresh:\n                failure_cases['high_confidence_fp'].append({\n                    'image': img_id,\n                    'confidence': pred[4],\n                    'best_iou': best_iou\n                })\n\nprint(\"=\"*70)\nprint(\"FAILURE CASE ANALYSIS\")\nprint(\"=\"*70)\nprint(f\"\\nTotal Missed Detections (FN): {len(failure_cases['missed_detections'])}\")\nprint(f\"Total False Positives (FP): {len(failure_cases['false_positives'])}\")\nprint(f\"Low Confidence True Positives (conf < {low_conf_thresh}): {len(failure_cases['low_confidence_tp'])}\")\nprint(f\"High Confidence False Positives (conf \u2265 {high_conf_thresh}): {len(failure_cases['high_confidence_fp'])}\")\n\nprint(f\"\\nMissed Detections Rate: {100*len(failure_cases['missed_detections'])/max(1,stats['total_gt_objects']):.2f}%\")\nprint(f\"False Positive Rate: {100*len(failure_cases['false_positives'])/max(1,stats['total_pred_objects']):.2f}%\")\n\n# Show sample failure cases\nif len(failure_cases['high_confidence_fp']) > 0:\n    print(f\"\\nSample High-Confidence False Positives:\")\n    for case in failure_cases['high_confidence_fp'][:5]:\n        print(f\"  {case['image']}: conf={case['confidence']:.3f}, best_IoU={case['best_iou']:.3f}\")\n\nif len(failure_cases['low_confidence_tp']) > 0:\n    print(f\"\\nSample Low-Confidence True Positives:\")\n    for case in failure_cases['low_confidence_tp'][:5]:\n        print(f\"  {case['image']}: conf={case['confidence']:.3f}, IoU={case['iou']:.3f}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 17. Visualize Best and Worst Cases with GT Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize best and worst performing images with GT comparison\nbest_images = per_image_stats[:3]\nworst_images = per_image_stats[-3:]\n\nfig, axes = plt.subplots(2, 3, figsize=(20, 14))\n\n# Plot best cases (top row)\nfor idx, stat in enumerate(best_images):\n    img_id = stat['image_id']\n    img_idx = image_ids.index(img_id)\n    img_path = os.path.join(VAL_IMAGES_DIR, img_id)\n    \n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    axes[0, idx].imshow(img)\n    \n    # Draw ground truth in green\n    for (x1, y1, x2, y2) in all_ground_truth[img_idx]:\n        rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=3, edgecolor='green', facecolor='none', label='GT')\n        axes[0, idx].add_patch(rect)\n    \n    # Draw predictions in red\n    for (x1, y1, x2, y2, conf) in all_predictions[img_idx]:\n        rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none', linestyle='--', label='Pred')\n        axes[0, idx].add_patch(rect)\n        axes[0, idx].text(x1, y1-5, f'{conf:.2f}', color='red', fontsize=10, weight='bold',\n                         bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n    \n    axes[0, idx].set_title(f\"BEST #{idx+1}: {img_id}\\nF1: {stat['f1']:.3f} | TP:{stat['tp']} FP:{stat['fp']} FN:{stat['fn']}\", \n                          fontsize=11, weight='bold')\n    axes[0, idx].axis('off')\n\n# Plot worst cases (bottom row)\nfor idx, stat in enumerate(worst_images):\n    img_id = stat['image_id']\n    img_idx = image_ids.index(img_id)\n    img_path = os.path.join(VAL_IMAGES_DIR, img_id)\n    \n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    axes[1, idx].imshow(img)\n    \n    # Draw ground truth in green\n    for (x1, y1, x2, y2) in all_ground_truth[img_idx]:\n        rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=3, edgecolor='green', facecolor='none', label='GT')\n        axes[1, idx].add_patch(rect)\n    \n    # Draw predictions in red\n    for (x1, y1, x2, y2, conf) in all_predictions[img_idx]:\n        rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='red', facecolor='none', linestyle='--', label='Pred')\n        axes[1, idx].add_patch(rect)\n        axes[1, idx].text(x1, y1-5, f'{conf:.2f}', color='red', fontsize=10, weight='bold',\n                         bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7))\n    \n    axes[1, idx].set_title(f\"WORST #{idx+1}: {img_id}\\nF1: {stat['f1']:.3f} | TP:{stat['tp']} FP:{stat['fp']} FN:{stat['fn']}\", \n                          fontsize=11, weight='bold')\n    axes[1, idx].axis('off')\n\nhandles = [plt.Line2D([0], [0], color='green', linewidth=3, label='Ground Truth'),\n           plt.Line2D([0], [0], color='red', linewidth=2, linestyle='--', label='Prediction')]\nfig.legend(handles=handles, loc='upper center', ncol=2, bbox_to_anchor=(0.5, 0.99), fontsize=12)\n\nplt.tight_layout(rect=[0, 0, 1, 0.97])\nplt.savefig(os.path.join(OUTPUT_DIR, 'best_worst_comparison.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nBest/Worst case comparison saved to {OUTPUT_DIR}/best_worst_comparison.png\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 18. Grid Cell Activation Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize grid cell activations for sample images\nsample_indices = [per_image_stats[0]['image_id'], per_image_stats[len(per_image_stats)//2]['image_id'], \n                  per_image_stats[-1]['image_id']]\n\nfig, axes = plt.subplots(3, 3, figsize=(18, 18))\n\nfor row, img_id in enumerate(sample_indices[:3]):\n    img_idx = image_ids.index(img_id)\n    img_path = os.path.join(VAL_IMAGES_DIR, img_id)\n    \n    # Load and preprocess image\n    img_tensor, params = letterbox_image(img_path, (IMG_SIZE, IMG_SIZE))\n    img_tensor_batch = img_tensor.unsqueeze(0).to(DEVICE)\n    \n    # Get model prediction\n    with torch.no_grad():\n        pred_grid = model(img_tensor_batch)[0].cpu().numpy()\n    \n    # Original image\n    img = cv2.imread(img_path)\n    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    axes[row, 0].imshow(img_rgb)\n    \n    # Draw GT boxes\n    for (x1, y1, x2, y2) in all_ground_truth[img_idx]:\n        rect = Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='green', facecolor='none')\n        axes[row, 0].add_patch(rect)\n    \n    axes[row, 0].set_title(f\"Original: {img_id}\", fontsize=10)\n    axes[row, 0].axis('off')\n    \n    # Objectness heatmap\n    objectness_map = pred_grid[:, :, 4]\n    im1 = axes[row, 1].imshow(objectness_map, cmap='hot', interpolation='nearest')\n    axes[row, 1].set_title(f\"Objectness Heatmap\\nMax: {objectness_map.max():.3f}, Mean: {objectness_map.mean():.3f}\", \n                          fontsize=10)\n    axes[row, 1].axis('off')\n    plt.colorbar(im1, ax=axes[row, 1], fraction=0.046, pad=0.04)\n    \n    # Overlay heatmap on image\n    img_resized = cv2.resize(img_rgb, (GRID_SIZE, GRID_SIZE))\n    axes[row, 2].imshow(img_resized, alpha=0.6)\n    im2 = axes[row, 2].imshow(objectness_map, cmap='hot', alpha=0.4, interpolation='nearest')\n    axes[row, 2].set_title(f\"Overlay (Grid {GRID_SIZE}x{GRID_SIZE})\", fontsize=10)\n    axes[row, 2].axis('off')\n\nplt.tight_layout()\nplt.savefig(os.path.join(OUTPUT_DIR, 'grid_activation_heatmaps.png'), dpi=150, bbox_inches='tight')\nplt.show()\n\nprint(f\"\\nGrid activation heatmaps saved to {OUTPUT_DIR}/grid_activation_heatmaps.png\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}